{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0416ce1a-29cf-44c1-b516-70c8ba116ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Javascript\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode, b64encode\n",
    "import numpy as np\n",
    "\n",
    "def init_camera():\n",
    "  \"\"\"Create objects and functions in HTML/JavaScript to access local web camera\"\"\"\n",
    "\n",
    "  js = Javascript('''\n",
    "\n",
    "    // global variables to use in both functions\n",
    "    var div = null;\n",
    "    var video = null;   // <video> to display stream from local webcam\n",
    "    var stream = null;  // stream from local webcam\n",
    "    var canvas = null;  // <canvas> for single frame from <video> and convert frame to JPG\n",
    "    var img = null;     // <img> to display JPG after processing with `cv2`\n",
    "\n",
    "    async function initCamera() {\n",
    "      // place for video (and eventually buttons)\n",
    "      div = document.createElement('div');\n",
    "      document.body.appendChild(div);\n",
    "\n",
    "      // <video> to display video\n",
    "      video = document.createElement('video');\n",
    "      video.style.display = 'block';\n",
    "      div.appendChild(video);\n",
    "\n",
    "      // get webcam stream and assing to <video>\n",
    "      stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
    "      video.srcObject = stream;\n",
    "\n",
    "      // start playing stream from webcam in <video>\n",
    "      await video.play();\n",
    "\n",
    "      // Resize the output to fit the video element.\n",
    "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
    "\n",
    "      // <canvas> for frame from <video>\n",
    "      canvas = document.createElement('canvas');\n",
    "      canvas.width = video.videoWidth;\n",
    "      canvas.height = video.videoHeight;\n",
    "      //div.appendChild(input_canvas); // there is no need to display to get image (but you can display it for test)\n",
    "\n",
    "      // <img> for image after processing with `cv2`\n",
    "      img = document.createElement('img');\n",
    "      img.width = video.videoWidth;\n",
    "      img.height = video.videoHeight;\n",
    "      div.appendChild(img);\n",
    "    }\n",
    "\n",
    "    async function takeImage(quality) {\n",
    "      // draw frame from <video> on <canvas>\n",
    "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
    "\n",
    "      // stop webcam stream\n",
    "      //stream.getVideoTracks()[0].stop();\n",
    "\n",
    "      // get data from <canvas> as JPG image decoded base64 and with header \"data:image/jpg;base64,\"\n",
    "      return canvas.toDataURL('image/jpeg', quality);\n",
    "      //return canvas.toDataURL('image/png', quality);\n",
    "    }\n",
    "\n",
    "    async function showImage(image) {\n",
    "      // it needs string \"data:image/jpg;base64,JPG-DATA-ENCODED-BASE64\"\n",
    "      // it will replace previous image in `<img src=\"\">`\n",
    "      img.src = image;\n",
    "      // TODO: create <img> if doesn't exists, \n",
    "      // TODO: use `id` to use different `<img>` for different image - like `name` in `cv2.imshow(name, image)`\n",
    "    }\n",
    "\n",
    "  ''')\n",
    "\n",
    "  display(js)\n",
    "  eval_js('initCamera()')\n",
    "\n",
    "def take_frame(quality=0.8):\n",
    "  \"\"\"Get frame from web camera\"\"\"\n",
    "\n",
    "  data = eval_js('takeImage({})'.format(quality))  # run JavaScript code to get image (JPG as string base64) from <canvas>\n",
    "\n",
    "  header, data = data.split(',')  # split header (\"data:image/jpg;base64,\") and base64 data (JPG)\n",
    "  data = b64decode(data)  # decode base64\n",
    "  data = np.frombuffer(data, dtype=np.uint8)  # create numpy array with JPG data\n",
    "\n",
    "  img = cv2.imdecode(data, cv2.IMREAD_UNCHANGED)  # uncompress JPG data to array of pixels\n",
    "\n",
    "  return img\n",
    "\n",
    "def show_frame(img, quality=0.8):\n",
    "  \"\"\"Put frame as <img src=\"data:image/jpg;base64,....\"> \"\"\"\n",
    "\n",
    "  ret, data = cv2.imencode('.jpg', img)  # compress array of pixels to JPG data\n",
    "\n",
    "  data = b64encode(data)  # encode base64\n",
    "  data = data.decode()  # convert bytes to string\n",
    "  data = 'data:image/jpg;base64,' + data  # join header (\"data:image/jpg;base64,\") and base64 data (JPG)\n",
    "\n",
    "  eval_js('showImage(\"{}\")'.format(data))  # run JavaScript code to put image (JPG as string base64) in <img>\n",
    "                                           # argument in `showImage` needs `\" \"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffb4d63-eb79-4440-a93c-8d3dd8f97945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(os.path.join(cv2.data.haarcascades, 'haarcascade_frontalface_default.xml'))\n",
    "\n",
    "# init JavaScript code\n",
    "init_camera()\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        img = take_frame()\n",
    "\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        #cv2_imshow(gray)  # it creates new image for every frame (it doesn't replace previous image) so it is useless\n",
    "        #show_frame(gray)  # it replace previous image\n",
    "\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "                cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        \n",
    "        #cv2_imshow(img)  # it creates new image for every frame (it doesn't replace previous image) so it is useless\n",
    "        show_frame(img)  # it replace previous image\n",
    "        \n",
    "    except Exception as err:\n",
    "        print('Exception:', err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731dcceb-c6b3-4f94-a544-a3531ae07e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install face_recognition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd5f57b-1ab9-479b-aab0-5051454ed50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "import os\n",
    "import re\n",
    "\n",
    "from datetime import datetime\n",
    "from google.colab.patches import cv2_imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352d5eb5-02d3-4ac5-9102-3e9736328389",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Training_images'\n",
    "images = []\n",
    "classNames = []\n",
    "myList = os.listdir(path)\n",
    "print(myList)\n",
    "for cl in myList:\n",
    "    curImg = cv2.imread(f'{path}/{cl}')\n",
    "    images.append(curImg)\n",
    "    classNames.append(os.path.splitext(cl)[0])\n",
    "print(classNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8591e37d-d6b8-49cf-a7fd-4707f67c3e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findEncodings(images):\n",
    "    encodeList = []\n",
    "    for img in images:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        encode = face_recognition.face_encodings(img)[0]\n",
    "        encodeList.append(encode)\n",
    "    return encodeList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c091642-0424-4bb7-9503-bfa206317cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def markAttendance(name):\n",
    "    with open('/content/Attendance.csv', 'r+') as f:\n",
    "        myDataList = f.readlines()\n",
    "\n",
    "\n",
    "        nameList = []\n",
    "        for line in myDataList:\n",
    "            entry = line.split(',')\n",
    "            nameList.append(entry[0])\n",
    "            if name not in nameList:\n",
    "                now = datetime.now()\n",
    "                dtString = now.strftime('%H:%M:%S')\n",
    "                f.writelines(f'\\n{name},{dtString}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed36651-6161-4554-9466-66bcdcfeee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodeListKnown = findEncodings(images)\n",
    "print('Encoding Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0198519e-1ec8-4e53-9a7f-7f58db709c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_camera()\n",
    "while True:\n",
    "  try:\n",
    "      #*******************cam 1****************************************\n",
    "\n",
    "    #cap1 =cv2.VideoCapture(\"http://192.168.0.179:8080/video\")\n",
    "      img1 = take_frame()\n",
    "  # img = captureScreen()\n",
    "      imgS1 = cv2.resize(img1, (0, 0), None, 0.25, 0.25)\n",
    "      imgS1 = cv2.cvtColor(imgS1, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "      facesCurFrame1 = face_recognition.face_locations(imgS1,model='R-CNN')\n",
    "      encodesCurFrame1 = face_recognition.face_encodings(imgS1,facesCurFrame1,num_jitters=2)\n",
    "\n",
    "      for encodeFace, faceLoc in zip(encodesCurFrame1, facesCurFrame1):\n",
    "          matches1 = face_recognition.compare_faces(encodeListKnown, encodeFace,tolerance=0.6)\n",
    "          faceDis1 = face_recognition.face_distance(encodeListKnown, encodeFace)\n",
    "  # print(faceDis)\n",
    "          matchIndex1 = np.argmin(faceDis1)\n",
    "          if matches1[matchIndex1]:\n",
    "              name_name1 = re.sub('\\d+', '', classNames[matchIndex1].upper())\n",
    "              name = classNames[matchIndex1].upper()\n",
    "  # print(name)\n",
    "              y11, x21, y21, x11 = faceLoc\n",
    "              y11, x21, y21, x11 = y11 * 4, x21 * 4, y21 * 4, x11 * 4\n",
    "              cv2.rectangle(img1, (x11, y11), (x21, y21), (0, 255, 0), 2)\n",
    "              cv2.rectangle(img1, (x11, y21 - 35), (x21, y21), (0, 255, 0), cv2.FILLED)\n",
    "              cv2.putText(img1, name_name1, (x11 + 6, y21 - 6), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 2)\n",
    "              markAttendance(name)\n",
    "          show_frame(img1)\n",
    "  except Exception as err:\n",
    "      print('Exception:', err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b277c25-f505-4def-ade9-5dd884f566c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
